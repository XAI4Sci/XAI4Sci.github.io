<!DOCTYPE HTML>
<!--
	Prologue by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>XAI4Sci: Explainable machine learning for sciences</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<div id="header">

				<div class="top">

					<!-- Logo 
						<div id="logo">
							<span class="image avatar48"><img src="images/ai.jpg" alt="" /></span>
							<h1 id="title">Jane Doe</h1>
							<p>Hyperspace Engineer</p>
						</div> -->

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="#top" id="top-link"><span class="icon solid fa-home">Home</span></a></li>
								<li><a href="#about" id="about-link"><span class="icon solid fa-info-circle">About</span></a></li>
								<li><a href="#schedule" id="schedule-link"><span class="icon solid fa-calendar">Schedule</span></a></li>
								<li><a href="#papers" id="papers-link"><span class="icon solid fa-newspaper">XAI4Sci Proceedings</span></a></li>
								<li><a href="#organizers" id="organizers-link"><span class="icon solid fa-users">Organizers</span></a></li>
							</ul>
						</nav>

				</div>

				<div class="middle">
				
					<h5><font color=#999999> Important dates:</font></h5>

					<p style="font-size: 18px; color: #999999">Submission Deadline:<br>December 1, 2023, 23:59 <a href="https://www.timeanddate.com/time/zones/est">EDT</a></p>
					<p style="font-size: 18px; color: #999999"">Review Deadline:<br>December 10, 2023, 23:59 <a href="https://www.timeanddate.com/time/zones/est">EDT</a></p>
					<p style="font-size: 18px; color: #999999"">Author notification:<br>December 11, 2023</p>
					<p style="font-size: 18px; color: #999999"">Workshop:<br>February 26, 2024</p>

				</div>
				
				<div class="bottom">

					<!-- Social Icons 
						<ul class="icons">
							<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon brands fa-github"><span class="label">Github</span></a></li>
							<li><a href="#" class="icon brands fa-dribbble"><span class="label">Dribbble</span></a></li>
							<li><a href="#" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
						</ul> -->

				</div>

			</div>

		<!-- Main -->
			<div id="main">

				<!-- Intro -->
					<section id="top" class="one dark cover">
						<div class="container">

							<header>
								<h2 class="alt strong">XAI4Sci: Explainable machine learning for sciences</h2>
									<!-- <span class="image avatar48"><img src="images/ai.jpg" alt="" /></span> -->
							<!-- <p>Ligula scelerisque justo sem accumsan diam quis<br />
								vitae natoque dictum sollicitudin elementum.</p> -->
							</header>

						</div>
					</section>

				<!-- About -->
					<section id="about" class="two">
						<div class="container">

							<header>
								<h2>About</h2>
							</header>

							<p>As the deployment of machine learning technology becomes increasingly common in applications of consequence, 
							such as medicine or science, the need for explanations of the system output has become a focus of great concern. 
							Unfortunately, many state-of-the-art models are opaque, making their use challenging from an explanation standpoint, 
							and current approaches to explaining these opaque models have stark limitations and have been the subject of serious
							criticism.</p>

							<p>The <i>XAI4Sci</i> workshop aims to bring together a diverse community of researchers and practitioners working at 
							the interface of science and machine learning to discuss the unique and pressing needs for explainable machine learning 
							models in support of science and scientific discovery. These needs include the ability to (1) leverage machine learning 
							as a tool to make measurements and perform other activities in a manner comprehensible to and verifiable by the working 
							scientists, and (2) enable scientists to utilize the explanations of the machine learning models in order to generate 
							new hypotheses and to further knowledge of the underlying science.</p>

							<p>The <i>XAI4Sci</i> workshop invites researchers to contribute short papers that demonstrate progress in the development 
							and application of explainable machine techniques to real-world problems in sciences (including but not limited to, 
							physics, materials science, earth science, cosmology, biology, chemistry, and  forensic science). The target audience 
							comprises members of the scientific community interested in explainable machine learning and researchers in the machine 
							learning community interested in scientific applications of explainable machine learning. The workshop will provide a 
							platform to facilitate a dialogue between these communities to discuss exciting open problems at the interface 
							of explainable machine learning and science. Leading researchers from both communities will cover state-of-the-art 
							techniques and set the stage for this workshop.</p>

							<h3>AAAI</h3>
							<p><span class="image right"><img style="width:6em;padding-left:0.5em;padding-top:.7em;" src="images/aaai-logo.png" alt="" /></span>
							The XAI4Sci: Explainable machine learning for sciences 2023 workshop will be held on February 26, 2024 at the at the Vancouver 
							Convention Centre &mdash; West Building in Vancouver, British Columbia, Canada as a part of the <a href="https://aaai.org/aaai-conference/">
							38th AAAI Conference on Artificial Intelligence</a> (AAAI-24). The AAAI-24 and all workshops are expected to take place in-person.

						</div>
					</section>

				<!-- Schedule -->
					<section id="schedule" class="three">
						<div class="container">

							<header>
								<h2>Schedule</h2>
							</header>

							<h4>9:00am - 10:30am Session</h4>
								<ul>
								<li>9:00 - 9:30: Katharina Beckh (Fraunhofer Institute for Intelligent Analysis and Information Systems)  <br>								 
									<button type="button" class="collapse">How prior knowledge can be utilized for explainable machine learning</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">Most explainability methods cannot provide insight beyond the given data which 
										necessitates additional information about the context. One solution is to integrate prior knowledge into the machine learning 
										pipeline. This talk presents three general approaches that integrate knowledge either into the machine learning pipeline, into 
										the explainability method or derive knowledge using explanations. Each approach is illustrated with an example from the sciences 
										to demonstrate the potential of prior knowledge integration.  </p>
									</div>  </li>
								
								<li>9:30 - 10:00: <a href="https://kevinwli.net/">Kevin Li</a> (Google DeepMind and University College London)<br>
									<button type="button" class="collapse">Uncertainty representation in cognitive systems</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">The ability to adequately handle uncertainty stands as a defining characteristic of intelligent 
										systems. When faced with novel experiences, our brains adapt to and seek patterns amidst unfamiliar signals. This talk explores how uncertainty 
										is represented in the human brain and its implications for learning and adaptation in artificial agents. In our recent work, we investigate 
										human limitations in modeling uncertainty in the environment and propose a bounded rational model to explain these behaviors. Furthermore, 
										we present a theoretical framework for understanding how neural systems could manage uncertainty, applying these insights to artificial agents 
										trained in both supervised and reinforcement learning contexts. Through this interdisciplinary approach, we gain deeper insights into the nature
										of uncertainty computation and its impact on intelligent behavior.</p>
									</div>  </li>	

								<li>10:00 - 10:30: P. Jonathon Phillips (Information Access Division, National Institute of Standards and Technology)<br>
									<button type="button" class="collapse">Challenges in using deep learning to model the face processing system in humans</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">Since the computer vision community adopted deep convolutional neural networks (DCNNs), 
										psychologists have explored DCNNs as models for the face processing system in humans (FPiH).  A DCNN-based face identification algorithm 
										takes a face image as input and returns an identity face code. A face code is a high-dimensional vector representing a face's identity. 
										Using these DCNNs engineered for face identification, psychologists investigated whether these DCNNs are models for the FPiH. In addition 
										to identifying faces, the FPiH performs numerous other tasks that include analyzing expressions, making social judgments, and assessing 
										the pose of a face. Surprisingly, these investigations found that the identity face codes produced by DCNNs encode information about a 
										face's expression, pose, and social judgment information (which are intrinsic to faces), but also information about illumination and the 
										type of sensor that took the face image (which are extrinsic to faces).  The talk will cover the experiments that led to these discoveries 
										and their impact on existing psychological and neurological models. Since existing DCNN architectures only model a portion of the FPiH, 
										the talk concludes by discussing the challenges of modeling the complete FPiH and designing experiments to confirm that deep learning models 
										are valid. (Joint work with David White of UNSW-Sydney.)</p>
									</div>  </li>
								</ul>
							
							<h4>10:30am - 11:00am Break (Light refreshments available near session rooms)</h4> <br>

							<h4>11:00am - 12:30pm Session</h4>
								<ul>
								<li>11:00 - 11:30: <a href="https://www.physics.wisc.edu/directory/cranmer-kyle/">Kyle Cranmer</a> (Physics Department, University of Wisconsin-Madison) <br>
									<button type="button" class="collapse">Injecting knowledge & extracting insight: promise and perils</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">I will discuss a few real world examples of projects focusing on inductive bias and architectural 
										choices that encode domain knowledge and for facilitate interpretability.  I will give an optimistic narrative of this approach and  highlight 
										some the pitfalls that can be encountered.</p>
									</div>  </li>
								
								<li>11:30 - 12:00 Lightning session for poster presenters </li>

								<li>12:00 - 12:30 Poster session </li>
								</ul>

							<h4>12:30pm - 2:00pm Lunch (On your own; no sponsored lunch provided) </h4> <br>

							<h4>2:00pm - 3:30pm Session</h4>
								<ul>
								<li>2:00 - 2:30: <a href="https://users.cs.duke.edu/~cynthia/">Cynthia Rudin</a> (Department of Electrical and Computer Engineering, Duke University) <br>
									<button type="button" class="collapse">We turned a black box into a scientific discovery and an interpretable model: a study in predicting breast 
										cancer years in advance</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">Breast cancer is the most commonly diagnosed cancer worldwide, impacting millions of women each year. 
											Population-wide breast cancer screening programs currently aim to regularly screen women over 40, normally annually. If we could predict whether 
											a given woman will develop cancer in the next 5 years, we could personalize this screening burden without increasing risk. 

											A recent black box model, Mirai, achieved impressive performance for 5-year breast cancer risk prediction, but was completely opaque in its 
											reasoning. While exploring Mirai, we discovered something surprising: Mirai's predictions were almost entirely based on localized dissimilarities 
											between left and right breasts. Guided by this insight, we showed that dissimilarity alone can predict whether a woman will develop breast cancer 
											in the next five years, developing an interpretable model that does just that.
											
											This work, entitled "AsymMirai: Interpretable Mammography-Based Deep Learning Model for 1- to 5-year Breast Cancer Risk Prediction, Radiology" 
											was recently accepted to Radiology. It is joint work with Jon Donnelly, Luke Moffett, Alina Barnett, Hari Trivedi, Fides Regina Schwartz, and 
											Joseph Lo. </p>
									</div>  </li>	

								<li>2:30 - 3:00: <a href="https://nyulangone.org/doctors/1548522964/peter-a-stella">Peter A. Stella, MD</a> (NYU Grossman School of Medicine) <br>
									<button type="button" class="collapse">Safety and explainability in clinical predictive models</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">AI derived predictive models offer the potential to significantly improve decision makingin clinical 
											medicine. In these high stakes applications, it is critical for that the models be both safe and perceived as safe.  We discuss how explainable 
											methods can address these twin issues effectively, with emphasis on several aspects of clinical data which tend to create specific patterns of 
											“error” in model building.</p>
									</div>  </li>

								<li>3:00 - 3:30: <a href="http://www.aaswathraman.com/">Aaswath P. Raman</a> (Samueli School of Engineering, University of California Los Angeles) <br>								 
									<button type="button" class="collapse">Explainable AI to both elucidate and optimize the design of complex optical materials and devices</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">Over the last thirty years the newly emerging fields of nanophotonics and metamaterials have 
										introduced new approaches to designing artificial materials that respond to electromagnetic waves in highly unusual ways, not achievable using 
										naturally accessible materials. This in turn has enabled a range of new device capabilities, including high-precision optical sensors, photonic 
										integrated circuits, new coatings for energy applications, and even exotic possibilities like invisibility cloaks. These unusual capabilities 
										arise from the ability to take conventional materials and by structuring or patterning them at length-scales that are either similar to, or 
										significantly smaller than, the wavelength of light one wishes to interact with. </p>

										<p style="margin:0px; padding-top:0px; margin-bottom: 10px;">In this context, a range of optimization and machine learning approaches have been 
										demonstrated to serve both as surrogate solvers that take a nanophotonic design and predict its optical response, as well as for inverse design, 
										where a nanophotonic design is optimized given an input target optical response. However, a fundamental challenge has emerged: the complex designs 
										that arise from these inverse design approaches can work well, but it is difficult to understand why they work and to further advance basic 
										understanding in the field. In this talk, I will introduce our work on using explainable AI approaches to both uncover why complex, often freeform, 
										structured material shapes are able to deliver particular optical responses. Additionally, I will show how an explainable AI approach (Deep SHAP) can 
										allow us to both understand the landscape navigated a conventional optimization algorithm, and to enhance its ability to perform inverse design. I 
										will conclude by discussing the potential for explainable AI approaches in the physical sciences more broadly, and how they might integrate with 
										physics-informed approaches such as PINNs to enable both improved optimization as well as scientific discovery.</p>
									</div>  </li>
								</ul>

								<h4>3:30pm - 4:00pm Break (Light refreshments available near session rooms)</h4> <br>

								<h4>4:00pm - 5:00pm Session</h4>
								<ul>
								<li>4:00 - 4:30: Yue Shi Lai (Nuclear Science Division, Lawrence Berkeley National Laboratory)<br>
									<button type="button" class="collapse">Explainable Generative-Adversarial Network for Particle Shower</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">We implement an explainable and physics-aware Generative Adversarial Network (GAN) that can 
										infer the underlying physics of high-energy particle collisions using the information encoded in the energy-momentum four-vectors of the 
										final state particles. This proof-of-concept shows how GAN can be constructed as a white box AI, which not only reproduces the final 
										distribution of particles, but also has the ability to reveal the physical laws needed to imitate the observation. Using Monte Carlo 
										generated collision events, we show how the underlying parton branching mechanism, i.e. the Altarelli-Parisi splitting function, the 
										ordering variable of the shower, and the scaling behavior can be recovered from the model. While the current work is primarily focused on 
										the perturbative physics of the parton shower, we foresee that white box construction of machine learning models can be a method to study 
										wide areas of high energy physics phenomenology that are currently difficult to address from first principles.</p>
									</div>  </li>
							
								<li>4:30 - 5:00 Panel discussion and closing remarks</li>
								</ul>
								
								
						</div>
					</section>

				<!-- Call for papers -->
					<section id="papers" class="two">
						<div class="container">

							<header>
								<h2>XAI4Sci Proceedings</h2>
							</header>

							<p>The XAI4Sci Workshop Proceedings (XAI4Sci Proceedings) is the written record of the scholarly work presented at the XAI4Sci: Explainable Machine 
							Learning for Sciences workshop. The proceedings cover the full range of experimental and theoretical research on applications of explainable machine 
							learning techniques to real-world problems in sciences, including but not limited to, physics, materials science, earth science, cosmology, biology, 
							chemistry, medicine, and forensic science; explainable machine learning; and applied machine learning.
							</p>	

							<table style="width:100%">
								<tr>
								  <td style="width:2%">1.</td>
								  <td style="width:94%">Rationalization of Synthesis-Structure Relationships of Nanoporous Materials using Aggregated SHAP Analysis  
									<em>by Elton Pan, Soonhyoung Kwon, Zach Jensen, Mingrou Xie, Rafael G&oacute;mez-Bombarelli, Manuel Moliner, Yuriy Roman, and Elsa Olivetti</em></td>
								  <td style="width:2%"><a href="papers/2024/1_rationalization_of_synthesis_structure_relationships.pdf"><span class="icon regular fa-file-pdf"> </span></a></td>
								  <!--<td style="width:2%"><span class="icon regular fa-image"> </span></td>-->
								</tr>
								<tr>
									<td>2.</td>
									<td>Data Science with LLMs and Interpretable Models  
									  <em>by Sebastian Bordt, Ben Lengerich, Harsha Nori, and Rich Caruana</em></td>
									<td><a href="papers/2024/2_data_science_with_llms_and_interpretable_models.pdf"><span class="icon regular fa-file-pdf"> </span></a></td>
									<!--<td><span class="icon regular fa-image"> </span></td>-->
								</tr>
								<tr>
									<td>3.</td>
									<td>Shapelet-based Model-agnostic Counterfactual Local Explanations for Time Series Classification  
									  <em>by Qi Huang, Wei Chen, Thomas B&auml;ck, and Niki van Stein</em></td>
									<td><a href="papers/2024/3_shapelet-based_model_agnostic_counterfactual_local_explanations_for_time_series_classification.pdf"><span class="icon regular fa-file-pdf"> </span></a></td>
									<!--<td><span class="icon regular fa-image"> </span></td>-->
								</tr>
								<tr>
									<td>4.</td>
									<td>Re-Discovering Tsallis distribution from High-Energy Physics data using Symbolic Regression 
									  <em>by Nour Makke and Sanjay Chawla</em></td>
									<td><a href="papers/2024/4_re_discovering_tsallis_distribution_from_high_energy_physics_data_using_symbolic_regression.pdf"><span class="icon regular fa-file-pdf"> </span></a></td>
									<!--<td><span class="icon regular fa-image"> </span></td>-->
								</tr>
								<tr>
									<td>5.</td>
									<td>Zero-shot Causal Graph Extrapolation from Text via LLMs
									  <em>by Alessandro Antonucci, Gregorio Piqu&eacute;, and Marco Zaffalon</em></td>
									<td><a href="papers/2024/5_zero_shot_causal_graph_extrapolation_from_text_via_llms.pdf"><span class="icon regular fa-file-pdf"> </span></a></td>
									<!--<td><span class="icon regular fa-image"> </span></td>-->
								</tr>
								tr>
									<td>6.</td>
									<td>Explainable Classification Techniques for Quantum Dot Device Measurements  
									  <em>by Daniel Schug, Tyler J. Kovach, M. A. Wolfe, Jared Benson, Sanghyeok Park, J. P. Dodson, J. Corrigan, M. A. Eriksson, and Justyna P. Zwolak</em></td>
									<td><a href="papers/2024/12_explainable_classification_techniques_for_quantum_dot_device_measurements.pdf"><span class="icon regular fa-file-pdf"> </span></a></td>
									<!--<td><span class="icon regular fa-image"> </span></td>-->
								</tr>
							  </table>
							
							<p><span class="image right"><img style="width:6em;padding-left:0.5em;padding-top:.7em;" src="images/MLST_Cover.jpg" alt="" /></span>
							We are pleased to announce that the XAI4Sci workshop has partnered with the Institute of Physics Publishing journal 
							<a href="https://iopscience.iop.org/journal/2632-2153">Machine Learning: Science and Technology</a> (MLST) to publish a focus issue 
							on <strong style="font-weight: bold;">Explainable machine learning in sciences</strong>. The focus issue will look to publish a selection of papers 
							at the interface of science and machine learning, focusing on tackling the unique and pressing needs for explainable machine learning models in support 
							of science and scientific discovery.</p>						
	
					</div>
				</section>

				<!-- Contact -->
					<section id="organizers" class="four">
						<div class="container">

							<header>
								<h2>Organizers</h2>
							</header>

							<ul class="features">
								<li>
									<img style="width:10em;" src="images/JPZ.png" alt=""/>
									<h3><a href="https://www.nist.gov/people/justyna-zwolak">Justyna Zwolak</a></br>NIST</h3>
								</li>
								<li>
									<img style="width:10em;" src="images/CSG.png" alt=""/>
									<h3><a href="https://www.nist.gov/people/craig-greenberg">Craig Greenberg</a></br>NIST</h3>
								</li>
								<li>
									<img style="width:10em;" src="images/RC.png" alt=""/>
									<h3><a href="https://www.microsoft.com/en-us/research/people/rcaruana/">Rich Caruana</a></br> Microsoft Research</h3>
								</li>
							</ul>

						

						</div>
					</section>

			</div>

		<!-- Footer -->
			<div id="footer">
				<p>For questions and comments, please contact: <a href="mailto:xai4sci2024@gmail.com">xai4sci2024@gmail.com</a></p>

			<!-- Copyright -->
				<ul class="copyright">
					<li>Copyright &copy; XAI4Sci.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
				</ul>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script> 
				var coll = document.getElementsByClassName("collapse");
				var i;

				for (i = 0; i < coll.length; i++) {
				coll[i].addEventListener("click", function() {
					this.classList.toggle("active");
					var content = this.nextElementSibling;
					if (content.style.display === "block") {
					content.style.display = "none";
					} else {
					content.style.display = "block";
					}
				});
				}

			</script>
	</body>
</html>