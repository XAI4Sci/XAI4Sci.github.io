<!DOCTYPE HTML>
<!--
	Prologue by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>XAI4Sci: Explainable machine learning for sciences</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<div id="header">

				<div class="top">

					<!-- Logo 
						<div id="logo">
							<span class="image avatar48"><img src="images/ai.jpg" alt="" /></span>
							<h1 id="title">Jane Doe</h1>
							<p>Hyperspace Engineer</p>
						</div> -->

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="#top" id="top-link"><span class="icon solid fa-home">Home</span></a></li>
								<li><a href="#about" id="about-link"><span class="icon solid fa-info-circle">About</span></a></li>
								<li><a href="#schedule" id="schedule-link"><span class="icon solid fa-calendar">Schedule</span></a></li>
								<li><a href="#papers" id="papers-link"><span class="icon solid fa-newspaper">Call for papers</span></a></li>
								<li><a href="#organizers" id="organizers-link"><span class="icon solid fa-users">Organizers</span></a></li>
							</ul>
						</nav>

				</div>

				<div class="middle">
				
					<h5><font color=#999999> Important dates:</font></h5>

					<p style="font-size: 18px; color: #999999">Submission Deadline:<br>December 1, 2023, 23:59 <a href="https://www.timeanddate.com/time/zones/est">EDT</a></p>
					<p style="font-size: 18px; color: #999999"">Review Deadline:<br>December 10, 2023, 23:59 <a href="https://www.timeanddate.com/time/zones/est">EDT</a></p>
					<p style="font-size: 18px; color: #999999"">Author notification:<br>December 11, 2023</p>
					<p style="font-size: 18px; color: #999999"">Workshop:<br>February 26, 2024</p>

				</div>
				
				<div class="bottom">

					<!-- Social Icons 
						<ul class="icons">
							<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon brands fa-github"><span class="label">Github</span></a></li>
							<li><a href="#" class="icon brands fa-dribbble"><span class="label">Dribbble</span></a></li>
							<li><a href="#" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
						</ul> -->

				</div>

			</div>

		<!-- Main -->
			<div id="main">

				<!-- Intro -->
					<section id="top" class="one dark cover">
						<div class="container">

							<header>
								<h2 class="alt strong">XAI4Sci: Explainable machine learning for sciences</h2>
									<!-- <span class="image avatar48"><img src="images/ai.jpg" alt="" /></span> -->
							<!-- <p>Ligula scelerisque justo sem accumsan diam quis<br />
								vitae natoque dictum sollicitudin elementum.</p> -->
							</header>

						</div>
					</section>

				<!-- About -->
					<section id="about" class="two">
						<div class="container">

							<header>
								<h2>About</h2>
							</header>

							<p>As the deployment of machine learning technology becomes increasingly common in applications of consequence, 
							such as medicine or science, the need for explanations of the system output has become a focus of great concern. 
							Unfortunately, many state-of-the-art models are opaque, making their use challenging from an explanation standpoint, 
							and current approaches to explaining these opaque models have stark limitations and have been the subject of serious
							criticism.</p>

							<p>The <i>XAI4Sci</i> workshop aims to bring together a diverse community of researchers and practitioners working at 
							the interface of science and machine learning to discuss the unique and pressing needs for explainable machine learning 
							models in support of science and scientific discovery. These needs include the ability to (1) leverage machine learning 
							as a tool to make measurements and perform other activities in a manner comprehensible to and verifiable by the working 
							scientists, and (2) enable scientists to utilize the explanations of the machine learning models in order to generate 
							new hypotheses and to further knowledge of the underlying science.</p>

							<p>The <i>XAI4Sci</i> workshop invites researchers to contribute short papers that demonstrate progress in the development 
							and application of explainable machine techniques to real-world problems in sciences (including but not limited to, 
							physics, materials science, earth science, cosmology, biology, chemistry, and  forensic science). The target audience 
							comprises members of the scientific community interested in explainable machine learning and researchers in the machine 
							learning community interested in scientific applications of explainable machine learning. The workshop will provide a 
							platform to facilitate a dialogue between these communities to discuss exciting open problems at the interface 
							of explainable machine learning and science. Leading researchers from both communities will cover state-of-the-art 
							techniques and set the stage for this workshop.</p>

							<h3>AAAI</h3>
							<p><span class="image right"><img style="width:6em;padding-left:0.5em;padding-top:.7em;" src="images/aaai-logo.png" alt="" /></span>
							The XAI4Sci: Explainable machine learning for sciences 2023 workshop will be held on February 26 or 27, 2024 at the at the Vancouver 
							Convention Centre &mdash; West Building in Vancouver, British Columbia, Canada as a part of the <a href="https://aaai.org/aaai-conference/">
							38th AAAI Conference on Artificial Intelligence</a> (AAAI-24). The AAAI-24 and all workshops are expected to take place in-person.

						</div>
					</section>

				<!-- Schedule -->
					<section id="schedule" class="three">
						<div class="container">

							<header>
								<h2>Schedule</h2>
							</header>

							<h4>9:00am - 10:30am Session</h4>
								<ul>
								<li>9:00 - 9:30: Katharina Beckh (Fraunhofer Institute for Intelligent Analysis and Information Systems)  <br>								 
									<button type="button" class="collapse">How prior knowledge can be utilized for explainable machine learning</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">Most explainability methods cannot provide insight beyond the given data which 
										necessitates additional information about the context. One solution is to integrate prior knowledge into the machine learning 
										pipeline. This talk presents three general approaches that integrate knowledge either into the machine learning pipeline, into 
										the explainability method or derive knowledge using explanations. Each approach is illustrated with an example from the sciences 
										to demonstrate the potential of prior knowledge integration.  </p>
									</div>  </li>
								
								<li>9:30 - 10:00: <a href="https://kevinwli.net/">Kevin Li</a> (Google DeepMind and University College London)<br>
									<button type="button" class="collapse">Uncertainty representation in cognitive systems</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">The ability to adequately handle uncertainty stands as a defining characteristic of intelligent 
										systems. When faced with novel experiences, our brains adapt to and seek patterns amidst unfamiliar signals. This talk explores how uncertainty 
										is represented in the human brain and its implications for learning and adaptation in artificial agents. In our recent work, we investigate 
										human limitations in modeling uncertainty in the environment and propose a bounded rational model to explain these behaviors. Furthermore, 
										we present a theoretical framework for understanding how neural systems could manage uncertainty, applying these insights to artificial agents 
										trained in both supervised and reinforcement learning contexts. Through this interdisciplinary approach, we gain deeper insights into the nature
										of uncertainty computation and its impact on intelligent behavior.</p>
									</div>  </li>	

								<li>10:00 - 10:30: <a href="https://users.cs.duke.edu/~cynthia/">Cynthia Rudin</a> (Department of Electrical and Computer Engineering, Duke University) <br>
									<button type="button" class="collapse">TBA</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">TBA</p>
									</div>  </li>
								</ul>
							
							<h4>10:30am - 11:00am Break (Light refreshments available near session rooms)</h4> <br>

							<h4>11:00am - 12:30pm Session</h4>
								<ul>
								<li>11:00 - 11:30: <a href="https://www.physics.wisc.edu/directory/cranmer-kyle/">Kyle Cranmer</a> (Physics Department, University of Wisconsin-Madison) <br>
									<button type="button" class="collapse">Injecting knowledge & extracting insight: promise and perils</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">I will discuss a few real world examples of projects focusing on inductive bias and architectural 
										choices that encode domain knowledge and for facilitate interpretability.  I will give an optimistic narrative of this approach and  highlight 
										some the pitfalls that can be encountered.</p>
									</div>  </li>
								
								<li>11:30 - 12:00 Lightning session for poster presenters </li>

								<li>12:00 - 12:30 Poster session </li>
								</ul>

							<h4>12:30pm - 2:00pm Lunch (On your own; no sponsored lunch provided) </h4> <br>

							<h4>2:00pm - 3:30pm Session</h4>
								<ul>
								<li>2:00 - 2:30: <a href="http://www.aaswathraman.com/">Aaswath P. Raman</a> (Samueli School of Engineering, University of California Los Angeles) <br>								 
									<button type="button" class="collapse">Explainable AI to both elucidate and optimize the design of complex optical materials and devices</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">Over the last thirty years the newly emerging fields of nanophotonics and metamaterials have 
										introduced new approaches to designing artificial materials that respond to electromagnetic waves in highly unusual ways, not achievable using 
										naturally accessible materials. This in turn has enabled a range of new device capabilities, including high-precision optical sensors, photonic 
										integrated circuits, new coatings for energy applications, and even exotic possibilities like invisibility cloaks. These unusual capabilities 
										arise from the ability to take conventional materials and by structuring or patterning them at length-scales that are either similar to, or 
										significantly smaller than, the wavelength of light one wishes to interact with. </p>

										<p style="margin:0px; padding-top:0px; margin-bottom: 10px;">In this context, a range of optimization and machine learning approaches have been 
										demonstrated to serve both as surrogate solvers that take a nanophotonic design and predict its optical response, as well as for inverse design, 
										where a nanophotonic design is optimized given an input target optical response. However, a fundamental challenge has emerged: the complex designs 
										that arise from these inverse design approaches can work well, but it is difficult to understand why they work and to further advance basic 
										understanding in the field. In this talk, I will introduce our work on using explainable AI approaches to both uncover why complex, often freeform, 
										structured material shapes are able to deliver particular optical responses. Additionally, I will show how an explainable AI approach (Deep SHAP) can 
										allow us to both understand the landscape navigated a conventional optimization algorithm, and to enhance its ability to perform inverse design. I 
										will conclude by discussing the potential for explainable AI approaches in the physical sciences more broadly, and how they might integrate with 
										physics-informed approaches such as PINNs to enable both improved optimization as well as scientific discovery.</p>
									</div>  </li>	

								<li>2:30 - 3:00: <a href="https://nyulangone.org/doctors/1548522964/peter-a-stella">Peter A. Stella, MD</a> (NYU Grossman School of Medicine) <br>
									<button type="button" class="collapse">Safety and explainability in clinical predictive models</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">AI derived predictive models offer the potential to significantly improve decision makingin clinical 
											medicine. In these high stakes applications, it is critical for that the models be both safe and perceived as safe.  We discuss how explainable 
											methods can address these twin issues effectively, with emphasis on several aspects of clinical data which tend to create specific patterns of 
											“error” in model building.</p>
									</div>  </li>

								<li>3:00 - 3:30: P. Jonathon Phillips (Information Access Division, National Institute of Standards and Technology)<br>
									<button type="button" class="collapse">Challenges in using deep learning to model the face processing system in humans</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">Since the computer vision community adopted deep convolutional neural networks (DCNNs), 
										psychologists have explored DCNNs as models for the face processing system in humans (FPiH).  A DCNN-based face identification algorithm 
										takes a face image as input and returns an identity face code. A face code is a high-dimensional vector representing a face's identity. 
										Using these DCNNs engineered for face identification, psychologists investigated whether these DCNNs are models for the FPiH. In addition 
										to identifying faces, the FPiH performs numerous other tasks that include analyzing expressions, making social judgments, and assessing 
										the pose of a face. Surprisingly, these investigations found that the identity face codes produced by DCNNs encode information about a 
										face's expression, pose, and social judgment information (which are intrinsic to faces), but also information about illumination and the 
										type of sensor that took the face image (which are extrinsic to faces).  The talk will cover the experiments that led to these discoveries 
										and their impact on existing psychological and neurological models. Since existing DCNN architectures only model a portion of the FPiH, 
										the talk concludes by discussing the challenges of modeling the complete FPiH and designing experiments to confirm that deep learning models 
										are valid. (Joint work with David White of UNSW-Sydney.)</p>
									</div>  </li>
								</ul>

								<h4>3:30pm - 4:00pm Break (Light refreshments available near session rooms)</h4> <br>

								<h4>4:00pm - 5:00pm Session</h4>
								<ul>
								<li>4:00 - 4:30: Yue Shi Lai (Nuclear Science Division, Lawrence Berkeley National Laboratory)<br>
									<button type="button" class="collapse">Explainable Generative-Adversarial Network for Particle Shower</button> 
									<div class="content"> 
										<p style="margin-bottom:10px; padding: 0px;">We implement an explainable and physics-aware Generative Adversarial Network (GAN) that can 
										infer the underlying physics of high-energy particle collisions using the information encoded in the energy-momentum four-vectors of the 
										final state particles. This proof-of-concept shows how GAN can be constructed as a white box AI, which not only reproduces the final 
										distribution of particles, but also has the ability to reveal the physical laws needed to imitate the observation. Using Monte Carlo 
										generated collision events, we show how the underlying parton branching mechanism, i.e. the Altarelli-Parisi splitting function, the 
										ordering variable of the shower, and the scaling behavior can be recovered from the model. While the current work is primarily focused on 
										the perturbative physics of the parton shower, we foresee that white box construction of machine learning models can be a method to study 
										wide areas of high energy physics phenomenology that are currently difficult to address from first principles.</p>
									</div>  </li>
							
								<li>4:40 - 5:00 Panel discussion</li>
								</ul>
								
								
						</div>
					</section>

				<!-- Call for papers -->
					<section id="papers" class="two">
						<div class="container">

							<header>
								<h2>Call for papers</h2>
							</header>

							<p>The XAI4Sci Workshop Proceedings (XAI4Sci Proceedings) is the written record of the scholarly work presented at the XAI4Sci: Explainable Machine 
							Learning for Sciences workshop. The proceedings cover the full range of experimental and theoretical research on applications of explainable machine 
							learning techniques to real-world problems in sciences, including but not limited to, physics, materials science, earth science, cosmology, biology, 
							chemistry, medicine, and forensic science; explainable machine learning; and applied machine learning.
							</p>	

							<h3>Submission instructions</h3>
							<p>The organizers welcome submissions of significant or final results as well as preliminary research results and discussions of works in progress. 
							The short conference papers (extended abstracts) should be up to 4 pages in length (excluding acknowledgments and references). The authors should 
							follow the guidelines and best practices from the AAAI conference (see the main <a href="https://aaai.org/aaai-conference/aaai-24-call-for-proposals/">
							conference website</a> for additional information). Also, please ensure that your paper is accessible to someone who is not an expert in your specific 
							research area. When preparing your manuscript, please use the LaTeX AAAI paper template (available <a href="https://xai4sci.github.io/resources/AuthorKit24.zip">
							here</a>). Please, make sure to include the additional <a href="https://xai4sci.github.io/resources/paper_info.docx">information sheet</a> with your 
							submission (a completed information sheet should be emailed to <a href="mailto:xai4sci2024@gmail.com">xai4sci2024@gmail.com</a>). We reserve the right to desk-reject submissions that do not conform to the required format or do not provide all required information. To
							submit your work, use the <a href="https://openreview.net/group?id=AAAI.org/2024/Workshop/XAI4Sci">OpenReview portal</a>.</p>

							<h3>Review process and publication</h3>
							<p>All contributed manuscripts will undergo a double-blind peer-review process. For each submitted paper the authors will be required to indicate 
							at least one person from the authors list who will serve as a reviewer. The reviewers will be asked to review three papers submitted to the workshop. 
							The accepted papers will be published on the workshop website. To prepare the final, camera-ready version of the paper, pleae use the LaTeX source 
							availabel <a href="https://xai4sci.github.io/resources/xai4sci_CameraReady.zip" style="font-weight: bold;">here</a>.</p>
							
							<p>Authors of selected papers submitted to the workshop will be invited to submit a full research paper for consideration in a special issue in 
							<a href="https://iopscience.iop.org/journal/2632-2153">Machine Learning: Science and Technology</a> focusing on explainable machine learning in sciences.</p>

							<h3>Publication FAQ</h3>
							<ul>
								<li style="font-weight: bold;">Q: Is the paper published in any proceedings?</li>
								<li style="list-style: none">A: All accepted papers will be hosted on the workshp website. In addition, we are discussing publishing a special focused
								issue in <a href="https://iopscience.iop.org/journal/2632-2153">Machine Learning: Science and Technology</a> where authors of select papers submitted 
								to the workshop would be invited to submit a full research paper.</li>
								<li style="font-weight: bold;">Q: Is there any additional cost associated with the acceptance of my paper?</li>
								<li style="list-style: none">No, there are no additional costs associated with our workshop. However, since the workshop is hosted by AAAI, all participants 
								must be registered for the AAAI-24 to attend the workshop talks and the poster sessions.</li>
							</ul>							
	
					</div>
				</section>

				<!-- Contact -->
					<section id="organizers" class="four">
						<div class="container">

							<header>
								<h2>Organizers</h2>
							</header>

							<ul class="features">
								<li>
									<img style="width:10em;" src="images/JPZ.png" alt=""/>
									<h3><a href="https://www.nist.gov/people/justyna-zwolak">Justyna Zwolak</a></br>NIST</h3>
								</li>
								<li>
									<img style="width:10em;" src="images/CSG.png" alt=""/>
									<h3><a href="https://www.nist.gov/people/craig-greenberg">Craig Greenberg</a></br>NIST</h3>
								</li>
								<li>
									<img style="width:10em;" src="images/RC.png" alt=""/>
									<h3><a href="http://theoryandpractice.org/">Rich Caruana</a></br> Microsoft Research</h3>
								</li>
							</ul>

						

						</div>
					</section>

			</div>

		<!-- Footer -->
			<div id="footer">
				<p>For questions and comments, please contact: <a href="mailto:xai4sci2024@gmail.com">xai4sci2024@gmail.com</a></p>

			<!-- Copyright -->
				<ul class="copyright">
					<li>Copyright &copy; XAI4Sci.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
				</ul>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script> 
				var coll = document.getElementsByClassName("collapse");
				var i;

				for (i = 0; i < coll.length; i++) {
				coll[i].addEventListener("click", function() {
					this.classList.toggle("active");
					var content = this.nextElementSibling;
					if (content.style.display === "block") {
					content.style.display = "none";
					} else {
					content.style.display = "block";
					}
				});
				}

			</script>
	</body>
</html>